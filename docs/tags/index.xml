<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tags on The Problem Architect</title>
    <link>https://1498saurav.github.io/the-problem-architect/tags/</link>
    <description>Recent content in Tags on The Problem Architect</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
        <atom:link href="https://1498saurav.github.io/the-problem-architect/tags/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>When AWS DMS Drops Your Partitioned Table: Deep Dive into DETACH PARTITION with PostgreSQL</title>
      <link>https://1498saurav.github.io/the-problem-architect/posts/dms-detach-partition/</link>
      <pubDate>Tue, 03 Jun 2025 20:00:00 +0530</pubDate>
      
      <guid>https://1498saurav.github.io/the-problem-architect/posts/dms-detach-partition/</guid>
      <description>&lt;h2 id=&#34;1-introduction-to-aws-dms--postgresql&#34;&gt;1. Introduction to AWS DMS &amp;amp; PostgreSQL&lt;/h2&gt;
&lt;p&gt;AWS Database Migration Service (DMS) is a managed service for migrating and replicating data with minimal downtime.&lt;br&gt;
With PostgreSQL as a source, DMS uses &lt;strong&gt;logical replication slots&lt;/strong&gt; and the WAL (Write-Ahead Log) to perform both &lt;strong&gt;full-load&lt;/strong&gt; and &lt;strong&gt;CDC (ongoing)&lt;/strong&gt; migrations.&lt;br&gt;
This ensures transactional consistency, since changes are read from the WAL and replayed on the target.&lt;/p&gt;
&lt;h2 id=&#34;2-full-load-parallelism-vs-single-threaded-cdc&#34;&gt;2. Full-Load Parallelism vs. Single-Threaded CDC&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Full-load&lt;/strong&gt; can be parallelized across tables or table slices using parameters like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;MaxFullLoadSubTasks&lt;/code&gt; (default 8, max 49) in &lt;strong&gt;TargetMetadata&lt;/strong&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;parallel-load&lt;/code&gt; options in table-mapping rules.
This greatly speeds up bulk data ingestion.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CDC&lt;/strong&gt;, however, preserves transaction order via a single stream through the &lt;strong&gt;SORTER&lt;/strong&gt;.&lt;br&gt;
It’s inherently single-threaded to maintain consistency—even if you’ve enabled multiple sub-tasks for full load.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3-capturing-ddl-captureddls--supported-statements&#34;&gt;3. Capturing DDL: &lt;code&gt;CaptureDdls&lt;/code&gt; &amp;amp; Supported Statements&lt;/h2&gt;
&lt;p&gt;By default, DMS sets &lt;code&gt;CaptureDdls=true&lt;/code&gt;, meaning &lt;strong&gt;all&lt;/strong&gt; DDL events in the WAL are captured and replayed on the target.
Supported DDLs include: &lt;code&gt;CREATE TABLE&lt;/code&gt;, &lt;code&gt;DROP TABLE&lt;/code&gt;, &lt;code&gt;ALTER TABLE&lt;/code&gt;, &lt;code&gt;RENAME&lt;/code&gt;, etc.
Thus, operations you intend as “metadata-only” (e.g. DETACH PARTITION) can inadvertently translate into destructive DDL on your target.&lt;/p&gt;
&lt;h2 id=&#34;4-how-dms-processes-ddl--dml&#34;&gt;4. How DMS Processes DDL &amp;amp; DML&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;SOURCE_CAPTURE&lt;/strong&gt;&lt;br&gt;
Reads WAL events (DML &amp;amp; DDL) via logical replication.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SORTER&lt;/strong&gt;&lt;br&gt;
Buffers and orders events in commit sequence. When in-memory buffers exceed &lt;code&gt;MemoryLimitTotal&lt;/code&gt;, it spills to &lt;strong&gt;swap files&lt;/strong&gt; on disk.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TARGET_APPLY&lt;/strong&gt;&lt;br&gt;
Applies the sorted stream to your target endpoint, including any captured DDL.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plain&#34; data-lang=&#34;plain&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[SORTER] I: Swap files in use—total storage used has exceeded the in-memory limit…
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;5-the-partition-detach-pitfall&#34;&gt;5. The Partition DETACH Pitfall&lt;/h2&gt;
&lt;p&gt;Consider this common sequence:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;ALTER&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;TABLE&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parent_table&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DETACH&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;PARTITION&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;child_2025_06&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;-- child_2025_06 now exists standalone
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;DROP&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;TABLE&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;child_2025_06&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;On PostgreSQL alone, this detaches then drops. But with DMS’s DDL capture:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;DETACH&lt;/strong&gt; emits a DROP TABLE for the child partition.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DMS&lt;/strong&gt; captures that DROP TABLE and replays it on the target—removing your data container unexpectedly. &lt;a href=&#34;https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.SupportedDDL.html&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Supported DDL&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;6-tuning-swap-files--sorter-efficiency&#34;&gt;6. Tuning Swap Files &amp;amp; Sorter Efficiency&lt;/h2&gt;
&lt;p&gt;When large or bursty change volumes hit the SORTER, it spills to disk. You can tune:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Setting&lt;/th&gt;
          &lt;th&gt;Purpose&lt;/th&gt;
          &lt;th&gt;Default&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;MemoryLimitTotal&lt;/td&gt;
          &lt;td&gt;Max MB for in-memory transaction cache before spill&lt;/td&gt;
          &lt;td&gt;1024 MB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;MemoryKeepTime&lt;/td&gt;
          &lt;td&gt;econds a transaction stays in memory before spilling&lt;/td&gt;
          &lt;td&gt;60 s&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;BatchApplyMemoryLimit&lt;/td&gt;
          &lt;td&gt;MB for batch-apply pre-processing&lt;/td&gt;
          &lt;td&gt;500 MB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;BatchSplitSize&lt;/td&gt;
          &lt;td&gt;Maximum change records per batch (0 = unlimited)&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.ChangeProcessingTuning.html&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Task Settings Params&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For extreme cases, see the AWS Knowledge Center on swap-file troubleshooting. &lt;a href=&#34;https://repost.aws/knowledge-center/dms-swap-files-consuming-space?utm_source=chatgpt.com&#34;target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Swap file fixes&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;7-separate-ddl-from-cdc&#34;&gt;7. Separate DDL from CDC&lt;/h2&gt;
&lt;p&gt;Use one task for full-load + CDC (no DDL) and another (or pg_dump) for schema changes only.
In your task JSON, set &amp;ldquo;CaptureDdls&amp;rdquo;: false under PostgreSQLSettings when you only want DML.
docs.aws.amazon.com&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>